{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Compute inverse solution\n\nThe inverse solution pipeline performs source reconstruction starting either\nfrom raw/epoched data (*.fif* format) specified by the user or from the output\nof the Preprocessing pipeline (cleaned raw data).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Annalisa Pascarella <a.pascarella@iac.cnr.it>\n\n# License: BSD (3-clause)\n\nimport os.path as op\n\nimport nipype.pipeline.engine as pe\n\nimport ephypype\nfrom ephypype.nodes import create_iterator, create_datagrabber\nfrom ephypype.datasets import fetch_omega_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us fetch the data first. It is around 675 MB download.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data_type = 'fif'\nbase_path = op.join(op.dirname(ephypype.__file__), '..', 'examples')\ndata_path = fetch_omega_dataset(base_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "then read the parameters for inverse solution from a json file\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import json  # noqa\nimport pprint  # noqa\ndata = json.load(open(\"params_inverse.json\"))\npprint.pprint({'inverse parameters': data})\n\nspacing = data['spacing']  # ico-5 vs oct-6\nsnr = data['snr']  # use smaller SNR for raw data\ninv_method = data['method']  # sLORETA, MNE, dSPM\nparc = data['parcellation']  # parcellation to use: 'aparc' vs 'aparc.a2009s'\n# noise covariance matrix filename template\nnoise_cov_fname = data['noise_cov_fname']\n\n# set sbj dir path, i.e. where the FS folfers are\nsubjects_dir = op.join(data_path, 'fsf')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we create our workflow and specify the `base_dir` which tells\nnipype the directory in which to store the outputs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# workflow directory within the `base_dir`\nsrc_reconstruction_pipeline_name = 'source_reconstruction_' + \\\n    inv_method + '_' + parc.replace('.', '')\n\nmain_workflow = pe.Workflow(name=src_reconstruction_pipeline_name)\nmain_workflow.base_dir = data_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we create a node to pass input filenames to DataGrabber from nipype\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "subject_ids = ['sub-0003']  # 'sub-0004', 'sub-0006'\nsession_ids = ['ses-0001']\ninfosource = create_iterator(['subject_id', 'session_id'],\n                             [subject_ids, session_ids])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and a node to grab data. The template_args in this node iterate upon\nthe values in the infosource node\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "template_path = '*%s/%s/meg/%s*rest*ica.fif'\ntemplate_args = [['subject_id', 'session_id', 'subject_id']]\ndatasource = create_datagrabber(data_path, template_path, template_args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ephypype creates for us a pipeline which can be connected to these\nnodes we created. The inverse solution pipeline is implemented by the\nfunction :func:`ephypype.pipelines.preproc_meeg.create_pipeline_source_reconstruction`,  # noqa\nthus to instantiate the inverse pipeline node, we import it and pass our\nparameters to it.\nThe inverse pipeline contains three nodes that wrap the MNE Python functions\nthat perform the source reconstruction steps.\n\nIn particular, the three nodes are:\n\n* :class:`ephypype.interfaces.mne.LF_computation.LFComputation` compute the\n  Lead Field matrix\n* :class:`ephypype.interfaces.mne.Inverse_solution.NoiseCovariance` computes\n  the noise covariance matrix\n* :class:`ephypype.interfaces.mne.Inverse_solution.InverseSolution` estimates\n  the time series of the neural sources on a set of dipoles grid\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from ephypype.pipelines.fif_to_inv_sol import create_pipeline_source_reconstruction  # noqa\ninv_sol_workflow = create_pipeline_source_reconstruction(\n    data_path, subjects_dir, spacing=spacing, inv_method=inv_method, parc=parc,\n    noise_cov_fname=noise_cov_fname)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then connect the nodes two at a time. First, we connect the two outputs\n(subject_id and session_id) of the infosource node to the datasource node.\nSo, these two nodes taken together can grab data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "main_workflow.connect(infosource, 'subject_id', datasource, 'subject_id')\nmain_workflow.connect(infosource, 'session_id', datasource, 'session_id')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similarly, for the inputnode of the preproc_workflow. Things will become\nclearer in a moment when we plot the graph of the workflow.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "main_workflow.connect(infosource, 'subject_id',\n                      inv_sol_workflow, 'inputnode.sbj_id')\nmain_workflow.connect(datasource, 'raw_file',\n                      inv_sol_workflow, 'inputnode.raw')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To do so, we first write the workflow graph (optional)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "main_workflow.write_graph(graph2use='colored')  # colored"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and visualize it. Take a moment to pause and notice how the connections\nhere correspond to how we connected the nodes.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from scipy.misc import imread  # noqa\nimport matplotlib.pyplot as plt  # noqa\nimg = plt.imread(op.join(data_path, src_reconstruction_pipeline_name, 'graph.png'))  # noqa\nplt.figure(figsize=(8, 8))\nplt.imshow(img)\nplt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we are now ready to execute our workflow.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "main_workflow.config['execution'] = {'remove_unnecessary_outputs': 'false'}\n\n# Run workflow locally on 3 CPUs\nmain_workflow.run(plugin='MultiProc', plugin_args={'n_procs': 1})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The output is the source reconstruction matrix stored in the workflow\ndirectory defined by `base_dir`. This matrix can be used as input of\nthe Connectivity pipeline.\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>To use this pipeline, we need a cortical segmentation of MRI\n  data, that could be provided by Freesurfer</p></div>\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}